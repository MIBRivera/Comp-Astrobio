{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v18n0F4awZIc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip -q install pathos \n",
    "#Multi processing\n",
    "!pip -q install tqdm \n",
    "# Progress bar\n",
    "!pip -q install TextBlob \n",
    "#Sentiment analysis\n",
    "!pip -q install \"dask[complete]\" \n",
    "# preprocessing and cross-validation\n",
    "!pip -q install dask_ml \n",
    "# scikit-learn drop-in replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathos.multiprocessing import ProcessPool\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_l1wxwn10Pr1"
   },
   "source": [
    "# Create a process pool\n",
    "\n",
    "Pools are a group of poccesses where you will send tasks. Inside you will define the number of processes to create. By defualt it will be number of CPU cores, however you **can** define more than that. \n",
    "\n",
    "Scheduling more processes than you have CPU cores can increase performance where the processes run into wait times or I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIgmDOjPxdJ8"
   },
   "outputs": [],
   "source": [
    "pool = ProcessPool(nodes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsQo-nam05hw"
   },
   "source": [
    "# Functions\n",
    "\n",
    "Map methods provided:\n",
    "\n",
    "\n",
    "    map         - blocking and ordered worker pool        [returns: list]\n",
    "    imap        - non-blocking and ordered worker pool    [returns: iterator]\n",
    "    uimap       - non-blocking and unordered worker pool  [returns: iterator]\n",
    "    amap        - asynchronous worker pool                [returns: object]\n",
    "\n",
    "Blocking: handles jobs in batches rather than 1 by 1\n",
    "\n",
    "Ordered: Batches must be completed in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sm3mZtOL0DL4",
    "outputId": "5cb52bd7-a3e0-4ed0-a754-3ac9524f3496"
   },
   "outputs": [],
   "source": [
    "#pool.map(function to run, data to run it on, other arguments )\n",
    "\n",
    "pool.map(pow, [1,2,3,4], [5,6,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjNaN7_m3T6L",
    "outputId": "41b8a6fe-16c6-4db2-b8b9-de4fcd18f41b"
   },
   "outputs": [],
   "source": [
    "#Iterate through the returned data using imap\n",
    "for x in pool.imap(pow, [1,2,3,4], [5,6,7,8]):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JMOQpbdc3hbW",
    "outputId": "0d1cd8e1-acc5-4a76-bba6-e82a82d45674"
   },
   "outputs": [],
   "source": [
    "# do an asynchronous map, then get the results\n",
    "import time\n",
    "\n",
    "results = pool.amap(pow, [1,2,3,4], [5,6,7,8])\n",
    "while not results.ready():\n",
    "    time.sleep(5); print(\".\", end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dhDD4OP6IOm"
   },
   "source": [
    "# Build your function\n",
    "First lets build a function that can take a line of text and produce the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adnqGeUoNL-Q"
   },
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    from textblob import TextBlob\n",
    "    blob = TextBlob(text)\n",
    "    score = blob.sentiment.polarity\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWVp5gY9N_Nk"
   },
   "source": [
    "Then we will need a function that will download the poems for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bk0GY2A5NTHo"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def download_poem(url):\n",
    "    poems = []\n",
    "    with urllib.request.urlopen(url) as f: \n",
    "        for line in f:\n",
    "            line = line.decode(\"utf-8\") \n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                poems.append(line)\n",
    "    return poems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oJXMDd2Jjp0"
   },
   "source": [
    "Let's check out what one of these poems look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6NTRf4SzJmRY",
    "outputId": "fa9c224f-c44c-462f-b0af-7bd461a23064"
   },
   "outputs": [],
   "source": [
    "test_url = 'https://raw.githubusercontent.com/okfn/openmilton/master/miltondata/texts/poems.txt'\n",
    "poem = download_poem(test_url)\n",
    "\n",
    "print(len(poem))\n",
    "print(poem[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq_aRF6IOF4o"
   },
   "source": [
    "Finally we will build out main function that puts the whole process together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVy1hTMq5IXF"
   },
   "outputs": [],
   "source": [
    "def process_poems(url):\n",
    "    scores = []\n",
    "    poems = []\n",
    "\n",
    "    import urllib\n",
    "    from textblob import TextBlob\n",
    "\n",
    "    with urllib.request.urlopen(url) as f: \n",
    "        for line in f:\n",
    "            line = line.decode(\"utf-8\") \n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                poems.append(line)\n",
    "\n",
    "    for line in poems:\n",
    "        blob = TextBlob(line)\n",
    "        score = blob.sentiment.polarity\n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KCuHjatRupqM",
    "outputId": "22edd801-56be-410a-a716-f8ccb90abf45"
   },
   "outputs": [],
   "source": [
    "print(len(process_poems(test_url)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EE78VNqKY14"
   },
   "source": [
    "Let's build a hard task, like having to download and process multiple poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vuxHqTFKeHJ",
    "outputId": "ffbfce6a-a668-4382-871e-6c532b41ece3"
   },
   "outputs": [],
   "source": [
    "urls = ['https://raw.githubusercontent.com/okfn/openmilton/master/miltondata/texts/poems.txt']\n",
    "\n",
    "#Duplicating the list to make it larger\n",
    "for _ in range(0,3):\n",
    "    urls += urls\n",
    "\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dac8UYGt_Jda"
   },
   "source": [
    "Now let's test how long it takes to process the sentiment for each line of our poems dataset\n",
    "\n",
    "We can use TQDM to show us the progress of any for-loop operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nNT03tbV_Rqk",
    "outputId": "cc99aa80-0beb-417a-e3a1-f2237a1fcd3b"
   },
   "outputs": [],
   "source": [
    "#Serial Processing\n",
    "scores = []\n",
    "\n",
    "for url in tqdm(urls, position=0 ): #position=0 forces the bars into the same line when printing\n",
    "    scores += process_poems(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Woel8UbhBuhs",
    "outputId": "e5826de6-43bf-4178-d071-fe8e8702989b"
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for score in tqdm(pool.uimap(process_poems, urls), total=len(urls), position=0):\n",
    "    scores += score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDHNt2P7vvOx"
   },
   "source": [
    "### Other multiprocessing cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06o-0uyzPCCZ",
    "outputId": "28b2d633-28e3-46ed-c212-a8cdcc8c3e1d"
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-dHIZopPKv7"
   },
   "outputs": [],
   "source": [
    "def square(lst):\n",
    "  arr = np.zeros_like(lst)\n",
    "  for i in range(lst.shape[0]):\n",
    "    for j in range(lst.shape[1]):\n",
    "      arr[i][j] = lst[i][j] ** 2\n",
    "  return arr\n",
    "\n",
    "array = np.random.randint(1, 9, (2**10, 10000))\n",
    "data = np.array_split(array, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHZeJMdnQVpA"
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjBgGWRiPYQM"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with mp.Pool(2) as p:\n",
    "  res = p.map(square, data)\n",
    "  p.close()\n",
    "  p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smzP4eUWQQ3U"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processes = []\n",
    "for i in range(2):\n",
    "  p = mp.Process(target=square, args=(data[i],))\n",
    "  processes.append(p)\n",
    "  p.start()\n",
    "  \n",
    "for p in processes: p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3qJfNR_6Qpsh",
    "outputId": "71ad3f2a-3214-48e7-e48c-3a941fbc234d"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "class test_cpu:\n",
    "    def f(self, x):  # changed\n",
    "        while True:\n",
    "            x * x\n",
    "\n",
    "    def load(self, cores):  # changed\n",
    "        print(\"utilizing %d cores\" % (cores / 2))\n",
    "        pool = Pool(10)\n",
    "        pool.map(self.f, range(6))  # changed\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"There are %d CPUs in your PC\" % multiprocessing.cpu_count())\n",
    "    cores_count = multiprocessing.cpu_count()\n",
    "    input_user = input(\"What do you want to test? type CPU, Memory or Both: \")\n",
    "    input_user.lower()\n",
    "    test_cpu_instance = test_cpu()  # changed\n",
    "    if input_user == \"cpu\":\n",
    "        test_cpu_instance.load(cores_count)  # changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tY775Oaz9lV"
   },
   "source": [
    "# Using dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bSNO4w8F0A_8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-colorblind')\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import recall_score, classification_report, precision_score, confusion_matrix, auc, accuracy_score, precision_recall_curve, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from scipy import ndimage\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('exoTest.csv').fillna(0)\n",
    "train_data = pd.read_csv('exoTrain.csv').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 13.91 MB\n",
      "Memory usage after optimization is: 6.25 MB\n",
      "Decreased by 55.1%\n",
      "Memory usage of dataframe is 124.12 MB\n",
      "Memory usage after optimization is: 62.04 MB\n",
      "Decreased by 50.0%\n"
     ]
    }
   ],
   "source": [
    "categ = {2: 1,1: 0}\n",
    "train_data.LABEL = [categ[item] for item in train_data.LABEL]\n",
    "test_data.LABEL = [categ[item] for item in test_data.LABEL]\n",
    "\n",
    "#Reduce memory\n",
    "def reduce_memory(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "test_data = reduce_memory(test_data)\n",
    "train_data = reduce_memory(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data\n",
    "x_train = train_data.drop([\"LABEL\"],axis=1)\n",
    "y_train = train_data[\"LABEL\"]   \n",
    "x_test = test_data.drop([\"LABEL\"],axis=1)\n",
    "y_test = test_data[\"LABEL\"]\n",
    "\n",
    "#Normalizing the data\n",
    "x_train = normalized = normalize(x_train)\n",
    "x_test = normalize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying of gaussian filter\n",
    "x_train = filtered = ndimage.filters.gaussian_filter(x_train, sigma=10)\n",
    "x_test = ndimage.filters.gaussian_filter(x_test, sigma=10)\n",
    "\n",
    "# Feature scaling -- after applying Gaussian filters, \n",
    "# make the Gaussian data have mean of 0 and variance of 1\n",
    "std_scaler = StandardScaler()\n",
    "x_train = scaled = std_scaler.fit_transform(x_train)\n",
    "x_test = std_scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 37\n",
      "Before OverSampling, counts of label '0': 5050 \n",
      "\n",
      "After OverSampling, counts of label '1': 5050\n",
      "After OverSampling, counts of label '0': 5049\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA with n_componenets\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=53)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "\n",
    "#Resampling as the data is highly unbalanced.\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
    "\n",
    "sm = SMOTEENN(random_state=27, sampling_strategy = 1.0, n_jobs=-1)\n",
    "x_train_res, y_train_res = sm.fit_resample(x_train, y_train) \n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.random import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.models import Sequential # initialize neural network library\n",
    "from keras.layers import Dense # build our layers library\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "from dask.dataframe import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(processes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'build_fn', 'warm_start', 'random_state', 'optimizer', 'loss', 'metrics', 'batch_size', 'validation_batch_size', 'verbose', 'callbacks', 'validation_split', 'shuffle', 'run_eagerly', 'epochs', 'class_weight'])\n",
      "Best: 0.980988 using {'batch_size': 10, 'epochs': 40, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.971581 (0.013084) with: {'batch_size': 5, 'epochs': 10, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.881572 (0.066372) with: {'batch_size': 5, 'epochs': 10, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.838202 (0.022921) with: {'batch_size': 5, 'epochs': 10, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.823646 (0.109212) with: {'batch_size': 5, 'epochs': 10, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.812853 (0.114895) with: {'batch_size': 5, 'epochs': 10, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.879988 (0.118685) with: {'batch_size': 5, 'epochs': 10, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.863254 (0.065804) with: {'batch_size': 5, 'epochs': 20, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.916526 (0.065058) with: {'batch_size': 5, 'epochs': 20, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.872661 (0.016681) with: {'batch_size': 5, 'epochs': 20, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.806714 (0.078174) with: {'batch_size': 5, 'epochs': 20, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.778097 (0.112600) with: {'batch_size': 5, 'epochs': 20, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.786613 (0.060275) with: {'batch_size': 5, 'epochs': 20, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.916328 (0.027532) with: {'batch_size': 5, 'epochs': 40, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.926527 (0.025423) with: {'batch_size': 5, 'epochs': 40, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.889989 (0.043573) with: {'batch_size': 5, 'epochs': 40, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.870878 (0.044247) with: {'batch_size': 5, 'epochs': 40, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.803347 (0.121314) with: {'batch_size': 5, 'epochs': 40, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.841965 (0.053351) with: {'batch_size': 5, 'epochs': 40, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.923755 (0.049862) with: {'batch_size': 10, 'epochs': 10, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.917616 (0.040084) with: {'batch_size': 10, 'epochs': 10, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.867809 (0.045414) with: {'batch_size': 10, 'epochs': 10, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.924646 (0.042616) with: {'batch_size': 10, 'epochs': 10, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.891970 (0.012226) with: {'batch_size': 10, 'epochs': 10, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.903753 (0.014986) with: {'batch_size': 10, 'epochs': 10, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.903852 (0.015838) with: {'batch_size': 10, 'epochs': 20, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.950688 (0.033037) with: {'batch_size': 10, 'epochs': 20, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.902268 (0.015762) with: {'batch_size': 10, 'epochs': 20, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.874938 (0.026065) with: {'batch_size': 10, 'epochs': 20, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.865432 (0.050133) with: {'batch_size': 10, 'epochs': 20, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.800970 (0.111727) with: {'batch_size': 10, 'epochs': 20, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.956035 (0.030348) with: {'batch_size': 10, 'epochs': 40, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.980988 (0.013473) with: {'batch_size': 10, 'epochs': 40, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.876819 (0.041713) with: {'batch_size': 10, 'epochs': 40, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.881671 (0.037537) with: {'batch_size': 10, 'epochs': 40, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.875730 (0.023649) with: {'batch_size': 10, 'epochs': 40, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.756709 (0.107966) with: {'batch_size': 10, 'epochs': 40, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.936924 (0.029685) with: {'batch_size': 20, 'epochs': 10, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.914348 (0.046189) with: {'batch_size': 20, 'epochs': 10, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.817408 (0.083996) with: {'batch_size': 20, 'epochs': 10, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.777404 (0.053248) with: {'batch_size': 20, 'epochs': 10, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.884939 (0.011137) with: {'batch_size': 20, 'epochs': 10, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.858897 (0.009461) with: {'batch_size': 20, 'epochs': 10, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.951678 (0.012531) with: {'batch_size': 20, 'epochs': 20, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.974057 (0.027722) with: {'batch_size': 20, 'epochs': 20, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.854243 (0.059230) with: {'batch_size': 20, 'epochs': 20, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.789088 (0.019841) with: {'batch_size': 20, 'epochs': 20, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.845232 (0.077693) with: {'batch_size': 20, 'epochs': 20, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.851470 (0.032844) with: {'batch_size': 20, 'epochs': 20, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "0.904248 (0.069299) with: {'batch_size': 20, 'epochs': 40, 'model__init': 'glorot_uniform', 'optimizer': 'rmsprop'}\n",
      "0.902169 (0.019670) with: {'batch_size': 20, 'epochs': 40, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.830082 (0.034959) with: {'batch_size': 20, 'epochs': 40, 'model__init': 'normal', 'optimizer': 'rmsprop'}\n",
      "0.842856 (0.031329) with: {'batch_size': 20, 'epochs': 40, 'model__init': 'normal', 'optimizer': 'adam'}\n",
      "0.878899 (0.074117) with: {'batch_size': 20, 'epochs': 40, 'model__init': 'uniform', 'optimizer': 'rmsprop'}\n",
      "0.772750 (0.071862) with: {'batch_size': 20, 'epochs': 40, 'model__init': 'uniform', 'optimizer': 'adam'}\n",
      "Elapsed time: 1114.2849378585815\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def create_model(optimizer='adam', init='uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=x_train_res.shape[1], kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(4, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "gridtrial = KerasClassifier(model=create_model, verbose=0)\n",
    "print(gridtrial.get_params().keys())\n",
    "\n",
    "# grid search epochs, batch size, kernel initializer, and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "epochs = [10, 20, 40]\n",
    "batches = [5, 10, 20]\n",
    "\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, model__init=init)\n",
    "\n",
    "tic = time.time()\n",
    "grid = GridSearchCV(estimator=gridtrial, param_grid=param_grid, scheduler=client)\n",
    "grid_result = grid.fit(x_train_res, y_train_res)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "toc = time.time()\n",
    "print(\"Elapsed time:\", toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best: 0.970195 using {'batch_size': 20, 'epochs': 10, 'model__init': 'glorot_uniform', 'optimizer': 'adam'}\n",
    "# Elapsed time: 3707.247061729431"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
